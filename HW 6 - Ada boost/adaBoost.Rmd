---
title: "Ada Boost"
author: "Rishabh Agnihotri"
date: "13 March 2015"
output: html_document
---

```{r, Import Libraries, echo = FALSE, include = FALSE}
if(!require("knitr")) install.packages("knitr")
opts_chunk$set(fig.width=8, fig.height=5, warning=FALSE, message=FALSE, include=TRUE, echo=TRUE, cache=TRUE, cache.comments=FALSE)

if(!require("assertthat")) install.packages("assertthat")
if(!require("rpart")) install.packages("rpart")
if(!require("dplyr")) install.packages("dplyr")
if(!require("mvtnorm")) install.packages("mvtnorm")
if(!require("ggplot2")) install.packages("ggplot2")
if(!require("randomForest")) install.packages("randomForest")
if(!require("reshape2")) install.packages("reshape2")
if(!require("grid")) install.packages("grid")

```

```{r, Get Data, echo = FALSE}

get.sigmaXY <- function(rhoXY, sdX, sdY){
  cov <- rhoXY * sdX *sdY
  matrix(c(sdX^2, cov, cov, sdY^2), nrow = 2)
}

get.data <- function(n1 = 100, n2 = 100, mu1, mu2, sd1, sd2, rhoXY, seed = 1000) {
  set.seed(seed)
  features1 <- rmvnorm(n1, mean = c(mu1[1], mu1[2]), 
                       sigma = get.sigmaXY(rhoXY[1], sd1[1], sd1[2]))
  features2 <- rmvnorm(n2, mean = c(mu2[1], mu2[2]), 
                       sigma = get.sigmaXY(rhoXY[2], sd2[1], sd2[2]))
  features <- rbind(features1, features2)
  label <- c(rep(-1, n1), rep(1, n2))
  index <- sample(1:(n1+n2))
  result <- list(features = as.data.frame(rbind(features1, features2))[index,], 
                 label = label[index])
  return(result)
}


draw.plot <- function(feat, lab) {
  data <- data.frame(feat, factor(lab))
  names(data) <- c("X1", "X2", "lab")
  ggplot(data, aes(x = X1, y = X2, fill = lab, color = lab)) +
    geom_point(alpha = 0.5, size = 3) +
    scale_color_manual(values = c("#AD0909", "#202D8F"))
  
}

#Create dataset
rhoXY <- c(0.8, 0.5)
sd1 <- c(2, 2)
mu1 <- c(15, 20)
sd2 <- c(1, 1.5)
mu2 <- c(15, 15)

train <- get.data(1000,1000, mu1, mu2, sd1, sd2, rhoXY, 2500)
test <- get.data(500,500, mu1, mu2, sd1, sd2, rhoXY, 200)

train.feat <- train[[1]]
train.lab <- train[[2]]
test.feat <- test[[1]]
test.lab <- test[[2]]

```


#1
##ADA BOOST FUNCTION

* This function runs in two modes. diagnostics = TRUE and diagnostics = FALSE.
* If diagnostics = TRUE, then every iteration is treated as training a tree and 
error rates are reported.
* If diagnostics = FALSE, the function returns predictions after training `iter`
number of trees.  

```{r}

ada.booster <- function(feat, lab, w, alpha.pred, iter = 10, 
                        diagnostics = FALSE, err.rate = c()) {
  
  #Predict labels at stump level 1
  tree = rpart(formula = lab~., data = feat, 
               weights = w, method = "class", control = c(maxdepth = 1) )
  pred = ifelse(predict(tree, type = "class") == "-1", -1, 1)
  
  #recalculate weights
  epsilon = (function() sum(w * ifelse(pred == lab, 0, 1))) ()
  alpha   = (function(x) 0.5 * log((1 - x) / x)) (epsilon)
  w       = w * exp((alpha * ifelse(pred != lab, 1, -1)))
  
  #calculate alpha * pred for iter
  alpha.pred = alpha.pred + ifelse(pred == "-1", -1, 1) * alpha
  
  #check diagnostics of function to decide output
  if(diagnostics == TRUE) err.rate <- c(err.rate, 
                                        sum(sign(alpha.pred) != lab) / length(lab)) 
  
  #Check if all weights are 0 i.e data is prefectly seperated
  zero.weights = (max(w) == 0 & min (w) == 0)
  
  if(iter == 1 | zero.weights) {
    if(zero.weights) print("Data is prefectly seperated. Quitting")
    
    if(diagnostics != TRUE) return(sign(alpha.pred))
    else return (err.rate)
  }
  else {
    ada.booster(feat, lab, w, alpha.pred, iter - 1, diagnostics, err.rate)
  }
}
```

#2
##Data
```{r, Display Data, echo=F}


plot1 <- draw.plot(train.feat, train.lab) + ggtitle("Train Data")
plot2 <- draw.plot(test.feat, test.lab) + ggtitle("Test Data")

pushViewport(viewport(layout = grid.layout(1, 2)))
print(plot1, vp = viewport(layout.pos.row = 1, layout.pos.col = 1))
print(plot2, vp = viewport(layout.pos.row = 1, layout.pos.col = 2))
```

#3
##ADA BOOST
```{r, ADA BOOST}
iter <- 100

#ADA BOOST
alpha.pred <- rep(0, length(train.lab))
w <- rep(1/length(train.lab), length(train.lab))
ad.err.rates <- ada.booster(train.feat, train.lab, w, alpha.pred, iter, diagnostics = TRUE)

err.df <- data.frame(iter = 1:iter, Ada_Boost_Error = ad.err.rates)
ggplot(err.df, aes(x = iter, y = Ada_Boost_Error)) + geom_point() + geom_line() +
  ggtitle("Error rates with test data")
  
```

#4
##COMPARE
###ADA BOOST, RANDOM FORESTS, BAGGING


```{r, COMPARE}

#Compare

#ADA BOOST
alpha.pred <- rep(0, length(train.lab))
w <- rep(1/length(test.lab), length(test.lab))
ad.err.rates <- ada.booster(test.feat, test.lab, w, alpha.pred, iter, diagnostics = TRUE)


#RANDOM FOREST
rf.err.rates <- sapply(X = 1:iter, FUN = function(v) {
  rf <- randomForest(train.feat, train.lab, ntree = v)
  pred <- sign(predict(rf , test.feat, type = "class"))
  sum(pred != test.lab) / length(test.lab)
})

#BAGGING
bag.train <- cbind(lab = factor(train.lab), train.feat)
bag.test <- cbind(lab = factor(test.lab), test.feat)

bag.err.rates <- sapply(X = 1:iter, FUN = function(v) {
  bag = adabag::bagging(lab ~., bag.train, mfinal=v)
  err = adabag::predict.bagging(bag, bag.test)$error
  err
})


```

```{r, Plot Errors}
#put errors in df
err.df <- data.frame(iter = 1:iter, ada.boost = ad.err.rates, 
                     rand.forest =rf.err.rates, bagging = bag.err.rates)
#melt df to long form
melt.error <- melt(err.df, id.vars = "iter", variable.name = "Algorithm", 
                   value.name = "Error")

#plot errors
ggplot(melt.error, aes(x = iter, y = Error, color = Algorithm)) + 
  geom_point() + 
  geom_line() +
  scale_color_manual(values = c("#1f78b4", "#33a02c", "#ff7f00")) +
  ggtitle("Test Error Comparison")

```

