} else if (type == "predict") {
errorCount <- NA
accuracy <- NA
}
# return the results
return(list(predictedClasses=predictedClasses,
prob=prob,
accuracy=accuracy,
errorCount=errorCount,
orderMatrix = neighbors))
}
datasetTest <- gen2c2dMixture(noObs=c(5000, 5000), seed=1234)
k <- 15  # odd number
p <- 2  # Manhattan (1), Euclidean (2) or Chebyshev (Inf)
testResults <- kNN_classifier(data=datasetTest[,1:2], trueClasses=dataset[,4],
memory=dataset[,1:2], k=k, p=2, type="predict")
kNN_classifier <- function(data, trueClasses, memory=NULL,
k=1, p=2, type="train") {
# test the inputs
library(assertthat)
not_empty(data); not_empty(trueClasses);
if (type=="train") {
assert_that(nrow(data)==length(trueClasses))
}
is.string(type); assert_that(type %in% c("train", "predict"))
is.count(k);
assert_that(p %in% c(1, 2, Inf))
if (type=="predict") {
assert_that(not_empty(memory) &
ncol(memory)==ncol(data) &
nrow(memory)==length(trueClasses))
}
# Compute the distance between each point and all others
noObs <- nrow(data)
# if we are making predictions on the test set based on the memory,
# we compute distances between each test observation and observations
# in our memory
if (type=="train") {
predictionId <- 1
distMatrix <- matrix(NA, noObs, noObs)
for (obs in 1:noObs) {
# getting the probe for the current observation
probe <- as.numeric(data[obs,])
probeExpanded <- matrix(rep(probe, each=noObs), nrow=noObs)
# computing distances between the probe and exemplars in the
# training data
if (p %in% c(1,2)) {
distMatrix[obs, ] <- (rowSums((abs(data -
probeExpanded))^p) )^(1/p)
} else if (p==Inf) {
distMatrix[obs, ] <- apply(abs(data - probeExpanded), 1, max)
}
}
} else if (type == "predict") {
predictionId <- 0
noMemory <- nrow(memory)
distMatrix <- matrix(NA, noObs, noMemory)
for (obs in 1:noObs) {
# getting the probe for the current observation
probe <- as.numeric(data[obs,])
probeExpanded <- matrix(rep(probe, each=noMemory), nrow=noMemory)
# computing distances between the probe and exemplars in the memory
if (p %in% c(1,2)) {
distMatrix[obs, ] <- (rowSums((abs(memory -
probeExpanded))^p) )^(1/p)
} else if (p==Inf) {
distMatrix[obs, ] <- apply(abs(memory - probeExpanded), 1, max)
}
}
}
# Sort the distances in increasing numerical order and pick the first
# k elements
neighbors <- apply(distMatrix, 1, order)
# Compute and return the most frequent class in the k nearest neighbors
prob <- predictedClasses <-  rep(NA, noObs)
for (obs in 1:noObs) {
prob[obs] <- mean(trueClasses[neighbors[(1+predictionId):
(k+predictionId), obs]])
if(prob[obs] > 0.5) {
predictedClasses[obs] <- 1
} else {
predictedClasses[obs] <- 0
}
}
# examine the performance, available only if training
if (type=="train") {
errorCount <- table(predictedClasses, trueClasses)
accuracy <- mean(predictedClasses==trueClasses)
} else if (type == "predict") {
errorCount <- NA
accuracy <- NA
}
# return the results
return(list(predictedClasses=predictedClasses,
prob=prob,
accuracy=accuracy,
errorCount=errorCount,
orderMatrix = neighbors))
}
distMat <- (apply( trainData[,1:2], 1, function(row) calc_dist(row, newData[,1:2], p))
)
dim(distMat)
orderMat <- as.matrix(t(apply(distMat, 1, order))[,1:k])
View(orderMat[1:10, 1:10])
View(orderMat[1:10, 1:k])
kNN_classifier <- function(data, trueClasses, memory=NULL,
k=1, p=2, type="train") {
# test the inputs
library(assertthat)
not_empty(data); not_empty(trueClasses);
if (type=="train") {
assert_that(nrow(data)==length(trueClasses))
}
is.string(type); assert_that(type %in% c("train", "predict"))
is.count(k);
assert_that(p %in% c(1, 2, Inf))
if (type=="predict") {
assert_that(not_empty(memory) &
ncol(memory)==ncol(data) &
nrow(memory)==length(trueClasses))
}
# Compute the distance between each point and all others
noObs <- nrow(data)
# if we are making predictions on the test set based on the memory,
# we compute distances between each test observation and observations
# in our memory
if (type=="train") {
predictionId <- 1
distMatrix <- matrix(NA, noObs, noObs)
for (obs in 1:noObs) {
# getting the probe for the current observation
probe <- as.numeric(data[obs,])
probeExpanded <- matrix(rep(probe, each=noObs), nrow=noObs)
# computing distances between the probe and exemplars in the
# training data
if (p %in% c(1,2)) {
distMatrix[obs, ] <- (rowSums((abs(data -
probeExpanded))^p) )^(1/p)
} else if (p==Inf) {
distMatrix[obs, ] <- apply(abs(data - probeExpanded), 1, max)
}
}
} else if (type == "predict") {
predictionId <- 0
noMemory <- nrow(memory)
distMatrix <- matrix(NA, noObs, noMemory)
for (obs in 1:noObs) {
# getting the probe for the current observation
probe <- as.numeric(data[obs,])
probeExpanded <- matrix(rep(probe, each=noMemory), nrow=noMemory)
# computing distances between the probe and exemplars in the memory
if (p %in% c(1,2)) {
distMatrix[obs, ] <- (rowSums((abs(memory -
probeExpanded))^p) )^(1/p)
} else if (p==Inf) {
distMatrix[obs, ] <- apply(abs(memory - probeExpanded), 1, max)
}
}
}
# Sort the distances in increasing numerical order and pick the first
# k elements
neighbors <- apply(distMatrix, 1, order)
# Compute and return the most frequent class in the k nearest neighbors
prob <- predictedClasses <-  rep(NA, noObs)
for (obs in 1:noObs) {
prob[obs] <- mean(trueClasses[neighbors[(1+predictionId):
(k+predictionId), obs]])
if(prob[obs] > 0.5) {
predictedClasses[obs] <- 1
} else {
predictedClasses[obs] <- 0
}
}
# examine the performance, available only if training
if (type=="train") {
errorCount <- table(predictedClasses, trueClasses)
accuracy <- mean(predictedClasses==trueClasses)
} else if (type == "predict") {
errorCount <- NA
accuracy <- NA
}
# return the results
return(list(predictedClasses=predictedClasses,
prob=prob,
accuracy=accuracy,
errorCount=errorCount,
distMatrix = distMatrix))
}
datasetTest <- gen2c2dMixture(noObs=c(5000, 5000), seed=1234)
k <- 15  # odd number
p <- 2  # Manhattan (1), Euclidean (2) or Chebyshev (Inf)
testResults <- kNN_classifier(data=datasetTest[,1:2], trueClasses=dataset[,4],
memory=dataset[,1:2], k=k, p=2, type="predict")
dim(testResults$distMatrix)
View(t(testResults$distMatrix)[1:10, 1:10])
View(distMat[1:10, 1:10])
library(gg)
library("ggplot2")
if(!require("knitr")) install.packages("knitr")
opts_chunk$set(fig.width=8, fig.height=5, warning=FALSE, message=FALSE, include=TRUE, echo=TRUE, cache=TRUE, cache.comments=FALSE)
if(!require("mvtnorm")) install.packages("mvtnorm")
if(!require("ggplot2")) install.packages("ggplot2")
if(!require("dplyr")) install.packages("dplyr")
if(!require("reshape2")) install.packages("reshape2")
setwd("c:/Users/Rishabh/Documents/BGSE Material/Sem2/14D005 Machine Learning/Seminar2/HW/")
source("dataset.R")
source("KNN_Classification.R")
noAnimals <- c(1000, 1000, 1000)
rho  <- c(-0.2, 0.8, 0.02)
sdXY <- list(c(2, 8), c(3.5, 12), c(1, 5))
muXY <- list(c(6, 30), c(12, 50), c(7,5))
animalsMem <- animals(rho, sdXY, muXY, noAnimals)
plotAnimals(animalsMem)
sdXY <- list(c(2, 8), c(6, 12), c(1, 5))
muXY <- list(c(6, 30), c(12, 50), c(7,5))
animalsMem <- animals(rho, sdXY, muXY, noAnimals)
plotAnimals(animalsMem)
plotAnimals(animalsMem) + ggtitle(label = "Prediction Points for Training Classifier")
sdXY <- list(c(2, 8), c(6, 8), c(1, 5))
animalsMem <- animals(rho, sdXY, muXY, noAnimals)
plotAnimals(animalsMem) + ggtitle(label = "Prediction Points for Training Classifier")
rho  <- c(-0.3, 0.4, 0.1)
sdXY <- list(c(2, 8), c(6, 8), c(1, 5))
muXY <- list(c(6, 30), c(12, 50), c(7,5))
animalsMem <- animals(rho, sdXY, muXY, noAnimals)
plotAnimals(animalsMem) + ggtitle(label = "Prediction Points for Training Classifier")
sdXY <- list(c(2, 8), c(6, 8), c(1, 10))
muXY <- list(c(6, 30), c(12, 50), c(7,5))
animalsMem <- animals(rho, sdXY, muXY, noAnimals)
# illustrating the data
plotAnimals(animalsMem) + ggtitle(label = "Prediction Points for Training Classifier")
sdXY <- list(c(2, 8), c(6, 8), c(10, 5))
muXY <- list(c(6, 30), c(12, 50), c(7,5))
animalsMem <- animals(rho, sdXY, muXY, noAnimals)
# illustrating the data
plotAnimals(animalsMem) + ggtitle(label = "Prediction Points for Training Classifier")
sdXY <- list(c(4, 6), c(6, 8), c(10, 5))
muXY <- list(c(6, 30), c(12, 50), c(7,5))
animalsMem <- animals(rho, sdXY, muXY, noAnimals)
# illustrating the data
plotAnimals(animalsMem) + ggtitle(label = "Prediction Points for Training Classifier")
sdXY <- list(c(4, 6), c(10, 8), c(10, 5))
muXY <- list(c(6, 30), c(12, 50), c(7,5))
animalsMem <- animals(rho, sdXY, muXY, noAnimals)
# illustrating the data
plotAnimals(animalsMem) + ggtitle(label = "Prediction Points for Training Classifier")
sdXY <- list(c(4, 6), c(10, 8), c(10, 7))
muXY <- list(c(6, 30), c(12, 50), c(7,5))
animalsMem <- animals(rho, sdXY, muXY, noAnimals)
# illustrating the data
plotAnimals(animalsMem) + ggtitle(label = "Prediction Points for Training Classifier")
muXY <- list(c(6, 30), c(10, 50), c(7,5))
animalsMem <- animals(rho, sdXY, muXY, noAnimals)
# illustrating the data
plotAnimals(animalsMem) + ggtitle(label = "Prediction Points for Training Classifier")
rho  <- c(-0.3, 0.4, 0.5)
sdXY <- list(c(4, 8), c(10, 8), c(10, 7))
muXY <- list(c(6, 30), c(12, 50), c(7,5))
animalsMem <- animals(rho, sdXY, muXY, noAnimals)
# illustrating the data
plotAnimals(animalsMem) + ggtitle(label = "Prediction Points for Training Classifier")
noAnimals <- c(500, 500, 500)
rho  <- c(-0.2, 0.8, 0.02)
sdXY <- list(c(2, 8), c(3.5, 12), c(1, 5))
muXY <- list(c(6, 30), c(12, 50), c(7,5))
animalsNew <- animals(rho, sdXY, muXY, noAnimals)
plotAnimals(animalsNew)
noAnimals <- c(500, 500, 500)
rho  <- c(-0.3, 0.4, 0.5)
muXY <- list(c(6, 30), c(12, 45), c(8,5))
sdXY <- list(c(4, 7), c(10, 8), c(10, 7))
animalsNew <- animals(rho, sdXY, muXY, noAnimals)
plotAnimals(animalsNew)
plotAnimals(animalsNew) + ggtitle(label = "New Points for Testing Classifier")
k <- 11
p <- 2
trained.output <- kNN_classifier(trainData = animalsMem, trueClasses = animalsMem[,3],
operation = "train", k = k, p = p)
trained.output$error
trained.output$accuracy
prediction <- kNN_classifier(trainData = animalsNew, memory = animalsMem,
trueClasses = animalsMem[,3],
k = k, p = p, operation = "predict")
#Test error
prediction$error
trained.output$accuracy
predict()
prediction$predictedClass
cbind(animalsNew, trueClass = prediction$predictedClass)
a <- cbind(animalsNew, trueClass = prediction$predictedClass)
a <- cbind(animalsNew[,1:2], trueClass = prediction$predictedClass)
install.packages("gcookbook")
library(gcookbook)
View(heightweight)
p <- ggplot(faithful, aes(x=eruptions, y=waiting))
p + geom_point() + stat_density2d()
View(a)
View(a)
ggplot(a, aes(y=weight, x=height, label = trueClass)) +
geom_dotplot(binaxis="y", binwidth=.5, stackdir="center")
p <- ggplot(faithful, aes(y=weight, x=height, label = trueClass))
p + geom_point() + stat_density2d()
p <- ggplot(animalsNew[,1:2], aes(y=weight, x=height, label = trueClass))
p + geom_point() + stat_density2d()
p <- ggplot(a, aes(y=weight, x=height, label = trueClass))
p + geom_point() + stat_density2d()
p <- ggplot(a, aes(y=weight, x=height, label = trueClass, colour=trueClass, fill=trueClass))
p + geom_point() + stat_density2d()
p <- ggplot(a, aes(y=weight, x=height, label = trueClass, colour=trueClass, fill=trueClass))
p + geom_point() + stat_density2d() +
scale_colour_brewer(palette="Set1")
p <- ggplot(a, aes(y=weight, x=height, label = trueClass, colour=trueClass, fill=trueClass))
p + geom_point() + stat_density2d(geom = "raster") +
scale_colour_brewer(palette="Set1")
p <- ggplot(a, aes(y=weight, x=height, label = trueClass, colour=trueClass, fill=trueClass))
p + geom_point() + stat_density2d(aes(fill=..density..), geom = "raster") +
scale_colour_brewer(palette="Set1")
p <- ggplot(a, aes(y=weight, x=height, label = trueClass, colour=trueClass, fill=trueClass))
p + geom_point() + stat_density2d(geom = "raster") +
scale_colour_brewer(palette="Set1")
p <- ggplot(a, aes(y=weight, x=height, label = trueClass, colour=trueClass, fill=trueClass))
p + geom_point() + stat_density2d() +
scale_colour_brewer(palette="Set1")
pred.df <- cbind(animalsNew[,1:2], trueClass = prediction$predictedClass)
pred.df <- cbind(animalsNew[,1:2], trueClass = prediction$predictedClass)
p <- ggplot(pred.df, aes(y=weight, x=height, label = trueClass, colour=trueClass, fill=trueClass))
p + geom_point() + stat_density2d() +
scale_colour_brewer(palette="Set1")
pred.df <- cbind(animalsNew[,1:2], trueClass = prediction$predictedClass)
p <- ggplot(pred.df, aes(y=weight, x=height, label = trueClass, colour=trueClass, fill=trueClass))
p + geom_point(alpha = 0.5) + stat_density2d() +
scale_colour_brewer(palette="Set1")
nce of the classifier with different values of k
k_seq <- seq.int(from = 1, to = 21, by = 2)
k_seq <- c(k_seq, 31, 41, 71, 101, 131, 161)
train_error <- sapply(k_seq, function(x) (1 -
kNN_classifier(trainData = animalsMem,
trueClasses = animalsMem[,3],
operation = "train", k = x, p = p)$accuracy))
predict_error <- sapply(k_seq, function(x) (1 -
kNN_classifier(trainData = animalsNew,
memory = animalsMem,
trueClasses = animalsMem[,3],
operation = "predict",
k = x, p = p)$accuracy))
#Create melted dataframe
error.DF <- data.frame(k = k_seq, train = train_error,
predict = predict_error) %>%
melt(id = "k", variable.name = "Sample", value.name = "Error")
#Plot error rates
ggplot(data = error.DF, aes(x = k, y = Error, color = Sample, fill = Sample,
label = as.numeric(round(Error * 100), 1))) +
geom_line() +
geom_point() +
geom_text(size = 4, hjust = 0, vjust = 0, color = "black") +
xlab("k -Number of nearest neighbors") +
ylab("Misclassification error") +
theme_bw(base_size = 14) +
scale_colour_brewer(palette="Set1")
k_seq <- seq.int(from = 1, to = 21, by = 2)
k_seq <- c(k_seq, 31, 41, 71, 101, 131, 161)
train_error <- sapply(k_seq, function(x) (1 -
kNN_classifier(trainData = animalsMem,
trueClasses = animalsMem[,3],
operation = "train", k = x, p = p)$accuracy))
k <- 11
p <- 2
trained.output <- kNN_classifier(trainData = animalsMem, trueClasses = animalsMem[,3],
operation = "train", k = k, p = p)
trained.output$error
trained.output$accuracy
prediction <- kNN_classifier(trainData = animalsNew, memory = animalsMem,
trueClasses = animalsMem[,3],
k = k, p = p, operation = "predict")
prediction$error
trained.output$accuracy
#Boundaries Roughly
pred.df <- cbind(animalsNew[,1:2], trueClass = prediction$predictedClass)
p <- ggplot(pred.df, aes(y=weight, x=height, label = trueClass, colour=trueClass, fill=trueClass))
p + geom_point(alpha = 0.5) + stat_density2d() +
scale_colour_brewer(palette="Set1")
k_seq <- seq.int(from = 1, to = 21, by = 2)
k_seq <- c(k_seq, 31, 41, 71, 101, 131, 161)
train_error <- sapply(k_seq, function(x) (1 -
kNN_classifier(trainData = animalsMem,
trueClasses = animalsMem[,3],
operation = "train", k = x, p = p)$accuracy))
p=2
k_seq <- seq.int(from = 1, to = 21, by = 2)
k_seq <- c(k_seq, 31, 41, 71, 101, 131, 161)
train_error <- sapply(k_seq, function(x) (1 -
kNN_classifier(trainData = animalsMem,
trueClasses = animalsMem[,3],
operation = "train", k = x, p = p)$accuracy))
predict_error <- sapply(k_seq, function(x) (1 -
kNN_classifier(trainData = animalsNew,
memory = animalsMem,
trueClasses = animalsMem[,3],
operation = "predict",
k = x, p = p)$accuracy))
error.DF <- data.frame(k = k_seq, train = train_error,
predict = predict_error) %>%
melt(id = "k", variable.name = "Sample", value.name = "Error")
ggplot(data = error.DF, aes(x = k, y = Error, color = Sample, fill = Sample,
label = as.numeric(round(Error * 100), 1))) +
geom_line() +
geom_point() +
geom_text(size = 4, hjust = 0, vjust = 0, color = "black") +
xlab("k -Number of nearest neighbors") +
ylab("Misclassification error") +
theme_bw(base_size = 14) +
scale_colour_brewer(palette="Set1")
k_min <- error.DF$k[which.min(error.DF$Error[error.DF$Sample == "train"])]
k_min
prediction <- kNN_classifier(trainData = animalsNew, memory = animalsMem,
trueClasses = animalsMem[,3],
k = k_min, p = p, operation = "predict")
error_rate <- 1 - prediction$accuracy
error_rate
round(error_rate *100, 2)
prediction$error
pred.df <- cbind(animalsNew[,1:2], trueClass = prediction$predictedClass)
p <- ggplot(pred.df, aes(y=weight, x=height, label = trueClass, colour=trueClass, fill=trueClass))
p + geom_point(alpha = 0.5) + stat_density2d() +
scale_colour_brewer(palette="Set1")
pred.df <- cbind(animalsNew[,1:2], trueClass = prediction$predictedClass)
p <- ggplot(pred.df, aes(y=weight, x=height, label = trueClass, colour=trueClass, fill=trueClass))
p + geom_point(alpha = 0.5) + stat_density2d() +
scale_colour_brewer(palette="Set1") +
ggtitle("Boundary Line")
if(!require("knitr")) install.packages("knitr")
opts_chunk$set(fig.width=8, fig.height=5, warning=FALSE, message=FALSE, include=TRUE, echo=TRUE, cache=TRUE, cache.comments=FALSE)
if(!require("mvtnorm")) install.packages("mvtnorm")
if(!require("ggplot2")) install.packages("ggplot2")
if(!require("dplyr")) install.packages("dplyr")
if(!require("doParallel")) install.packages("doParallel")
train.data <- read.csv("training.csv", header = F)
trueClasses <- train.data[,1]
train.data <- train.data[,-1]
#Test Data
test.data <- read.csv("test.csv")
k = 1
cl <- makeCluster(cores)
registerDoParallel(cl)
clusterExport(cl, c("train.data", "calc_dist", "test.data"))
system.time(dist_matrix <- parApply(cl, train.data, 1, function(row) (calc_dist(row, test.data))))
stopCluster(cl)
cl <- makeCluster(cores)
cores <- detectCores()
k_seq <- seq.int(from = 1, to = 51, by = 2)
k_seq <- c(k_seq, 71, 91, 111, 131, 151, 171)
cl <- makeCluster(cores)
registerDoParallel(cl)
cl <- makeCluster(cores)
registerDoParallel(cl)
clusterExport(cl, c("train.data", "calc_dist", "test.data"))
system.time(dist_matrix <- parApply(cl, train.data, 1, function(row) (calc_dist(row, test.data))))
source("KNN_Classification.R")
k_seq <- seq.int(from = 1, to = 51, by = 2)
k_seq <- c(k_seq, 71, 91, 111, 131, 151, 171)
cl <- makeCluster(cores)
registerDoParallel(cl)
error_rate <- rep(NA, length(k_seq))
for(i in 1:length(k_seq)) {
)
cl <- makeCluster(cores)
registerDoParallel(cl)
clusterExport(cl, c("train.data", "calc_dist", "test.data"))
system.time(dist_matrix <- parApply(cl, train.data, 1, function(row) (calc_dist(row, test.data))))
dim(dist_matrix)
cl <- makeCluster(cores)
registerDoParallel(cl)
clusterExport(cl, c("dist_matrix"))
system.time(orderMat <- as.matrix(t(apply(dist_matrix, 1, order))[1:k]))
stopCluster(cl)
dim(orderMat)
dim(dist_matrix)
cl <- makeCluster(cores)
registerDoParallel(cl)
clusterExport(cl, c("dist_matrix"))
system.time(orderMat <- as.matrix(t(parApply(cl, dist_matrix, 1, order))[1:k]))
stopCluster(cl)
cl <- makeCluster(cores)
registerDoParallel(cl)
clusterExport(cl, c("dist_matrix"))
system.time(orderMat <- as.matrix(t(parApply(cl, dist_matrix, 1, order))[,1:k]))
stopCluster(cl)
test.data <- read.csv("test.csv", header = F)
cl <- makeCluster(cores)
registerDoParallel(cl)
clusterExport(cl, c("train.data", "calc_dist", "test.data"))
system.time(dist_matrix <- parApply(cl, train.data, 1,
function(row) (calc_dist(row, test.data))))
stopCluster(cl)
cl <- makeCluster(cores)
registerDoParallel(cl)
clusterExport(cl, c("k.orderMat", "trueClasses"))
system.time(predictedClass <- as.character(parApply(cl, k.orderMat, 1,
function(x) names(which.max(table(trueClasses[x]))))))
cl <- makeCluster(cores)
registerDoParallel(cl)
clusterExport(cl, c("dist_matrix"))
system.time(orderMat <- as.matrix(t(parApply(cl, dist_matrix, 1, order))[,1:k]))
stopCluster(cl)
cl <- makeCluster(cores)
registerDoParallel(cl)
clusterExport(cl, c("orderMat", "trueClasses"))
system.time(predictedClass <- as.character(
parApply(cl, orderMat, 1, function(x) names(which.max(table(trueClasses[x]))))))
stopCluster(cl)
table(predictedClass)
write.csv(x = predictedClass, file = "predictedDigits.csv")
